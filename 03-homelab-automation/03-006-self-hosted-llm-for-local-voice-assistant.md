---
title: 'Self-Hosted LLM for Local Voice Assistant'
number: '03-006'
category: 'homelab-automation'
difficulty: 'Hard'
time_commitment: 'Months'
target_skills: 'Docker, GPU Passthrough, LocalAI/Ollama, Home Assistant'
status: 'Not Started'
---

# Self-Hosted LLM for Local Voice Assistant

## Description

Set up a local LLM (like Llama 3 or Mistral) on your homelab server using GPU passthrough. Integrate
it with Home Assistant's voice pipeline to create a completely private, offline voice assistant.

## Exit Criteria

- [ ] Define what done looks like for this project

## Progress

- [ ] Initial research
- [ ] Implementation
- [ ] Documentation
